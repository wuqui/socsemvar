[
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "overview",
    "section": "",
    "text": "models = load_models(['2019', '2020'], models_dir='../../models')\nmodels\n\n{'2019': &lt;gensim.models.word2vec.Word2Vec&gt;,\n '2020': &lt;gensim.models.word2vec.Word2Vec&gt;}\n\n\nVocabulary sizes for the two models before Procrustes alignment:\n\npd.DataFrame(\n    columns=['Model', 'VocabSize'],\n    data=[\n    ['2019', f\"{len(models['2019'].wv.key_to_index):,}\"],\n    ['2020', f\"{len(models['2020'].wv.key_to_index):,}\"],\n])\n\n\n\n\n\n\n\n\n\nModel\nVocabSize\n\n\n\n\n0\n2019\n252,564\n\n\n1\n2020\n277,707\n\n\n\n\n\n\n\n\n\nsmart_procrustes_align_gensim(models['2019'], models['2020'])\n\n190756 190756\n190756 190756\n\n\n&lt;gensim.models.word2vec.Word2Vec&gt;\n\n\nIntersecting vocabulary size after alignment:\n\npd.DataFrame(\n    columns=['Model', 'VocabSize'],\n    data=[\n    ['2019', f\"{len(models['2019'].wv.key_to_index):,}\"],\n    ['2020', f\"{len(models['2020'].wv.key_to_index):,}\"],\n])\n\n\n\n\n\n\n\n\n\nModel\nVocabSize\n\n\n\n\n0\n2019\n190,756\n\n\n1\n2020\n190,756\n\n\n\n\n\n\n\n\nMeasuring semantic distances (~ cosine distance) between the 2019 and the 2020 model for all words contained in the aligned vocabulary.\n\ndistances = measure_distances(models['2019'], models['2020'])\n\n\n20 words that show the highest semantic distance between 2019 and 2020. This output is presented in Table 2 in the paper.\n\nget_change_candidates(20, distances)\n\n\n\n\n\n\n\n\n\nlex\ndist_sem\n\n\n\n\n0\nlockdowns\n1.02\n\n\n1\nmaskless\n1.00\n\n\n2\nsunsetting\n1.00\n\n\n3\nchilde\n0.98\n\n\n4\nmegalodon\n0.98\n\n\n5\nnewf\n0.96\n\n\n6\ncorona\n0.93\n\n\n7\nfiltrate\n0.92\n\n\n8\nchaz\n0.90\n\n\n9\nklee\n0.89\n\n\n10\nrona\n0.89\n\n\n11\ncerb\n0.87\n\n\n12\nrittenhouse\n0.87\n\n\n13\nvacuo\n0.86\n\n\n14\nmoderna\n0.84\n\n\n15\npandemic\n0.84\n\n\n16\nspreader\n0.84\n\n\n17\ndistancing\n0.83\n\n\n18\nsars\n0.83\n\n\n19\nquarantines\n0.82\n\n\n\n\n\n\n\n\nExtended list for the Appendix (Table 3)\n\nget_change_candidates(50, distances, propNouns=False)\n\n\n\n\n\n\n\n\n\nlex\ndist_sem\n\n\n\n\n0\nlockdowns\n1.02\n\n\n1\nmaskless\n1.00\n\n\n2\nsunsetting\n1.00\n\n\n3\nnewf\n0.96\n\n\n4\ncorona\n0.93\n\n\n5\nfiltrate\n0.92\n\n\n6\nchaz\n0.90\n\n\n7\nrona\n0.89\n\n\n8\ncerb\n0.87\n\n\n9\nvacuo\n0.86\n\n\n10\nmoderna\n0.84\n\n\n11\npandemic\n0.84\n\n\n12\nspreader\n0.84\n\n\n13\ndistancing\n0.83\n\n\n14\nsars\n0.83\n\n\n15\nquarantines\n0.82\n\n\n16\nyada\n0.82\n\n\n17\nrecounts\n0.82\n\n\n18\nalway\n0.81\n\n\n19\nyadda\n0.80\n\n\n20\npandemics\n0.80\n\n\n21\npansies\n0.79\n\n\n22\ntosser\n0.79\n\n\n23\nbipoc\n0.79\n\n\n24\nventilators\n0.79\n\n\n25\nbudging\n0.79\n\n\n26\ndiys\n0.78\n\n\n27\nthst\n0.78\n\n\n28\nflyweight\n0.77\n\n\n29\nyeap\n0.77\n\n\n30\nmrna\n0.77\n\n\n31\ntiktoks\n0.77\n\n\n32\nbuuuut\n0.76\n\n\n33\ncoomer\n0.76\n\n\n34\nunfortunatly\n0.75\n\n\n35\nanywho\n0.75\n\n\n36\nquarantining\n0.74\n\n\n37\nventi\n0.74\n\n\n38\nwebrip\n0.74\n\n\n39\nobvi\n0.74\n\n\n40\nfkin\n0.74\n\n\n41\nmodus\n0.73\n\n\n42\ntink\n0.73\n\n\n43\nduplicating\n0.73\n\n\n44\nretinoids\n0.73\n\n\n45\nparasol\n0.72\n\n\n46\ncopypastas\n0.72\n\n\n47\nexcercise\n0.72\n\n\n48\nnewbies\n0.72\n\n\n49\nmers\n0.72",
    "crumbs": [
      "overview"
    ]
  },
  {
    "objectID": "overview.html#evaluative-dimension-good-vs-bad",
    "href": "overview.html#evaluative-dimension-good-vs-bad",
    "title": "overview",
    "section": "evaluative dimension: good vs bad",
    "text": "evaluative dimension: good vs bad\n\npole_words = ['good', 'bad']\n\nproj_sims = get_axis_sims(lexs, models, pole_words, k=10)\nproj_sims = aggregate_proj_sims(proj_sims)\nproj_sims_melted = proj_sims.melt(id_vars=['lex', 'SimDiff'], var_name='model', value_name='SemSim')\nsem_axis_evaluative_plot = plot_sem_axis(proj_sims_melted,  pole_words)\nsem_axis_evaluative_plot",
    "crumbs": [
      "overview"
    ]
  },
  {
    "objectID": "overview.html#mft-based-dimension-loyalty-vs-betrayal",
    "href": "overview.html#mft-based-dimension-loyalty-vs-betrayal",
    "title": "overview",
    "section": "MFT-based dimension: loyalty vs betrayal",
    "text": "MFT-based dimension: loyalty vs betrayal\n\npole_words = ['loyalty', 'betrayal']\n\nproj_sims = get_axis_sims(lexs, models, pole_words, k=10)\nproj_sims = aggregate_proj_sims(proj_sims)\nproj_sims_melted = proj_sims.melt(id_vars=['lex', 'SimDiff'], var_name='model', value_name='SemSim')\nsem_axis_evaluative_plot = plot_sem_axis(proj_sims_melted,  pole_words)\nsem_axis_evaluative_plot",
    "crumbs": [
      "overview"
    ]
  },
  {
    "objectID": "overview.html#common-neighbours",
    "href": "overview.html#common-neighbours",
    "title": "overview",
    "section": "common neighbours",
    "text": "common neighbours\n\nnbs_vecs_2d = dim_red_nbs_vecs(nbs_vecs, perplexity=0.1)\nnbs_sim = (nbs_vecs_2d\n    .groupby('subreddit')\n    .apply(lambda df: df.nlargest(10, 'sim'))\n    .reset_index(drop=True)\n)\n\n/var/folders/gp/dw55jb3d3gl6jn22rscvxjm40000gn/T/ipykernel_2126/3553513103.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  .apply(lambda df: df.nlargest(10, 'sim'))\n\n\n\nmap_sims_plot = (alt.Chart(nbs_sim).mark_text().encode(\n        x='x_tsne:Q',\n        y='y_tsne:Q',\n        text='lex',\n        color='subreddit:N'\n    ))\n\nmap_sims_plot",
    "crumbs": [
      "overview"
    ]
  },
  {
    "objectID": "overview.html#differences-in-neighbours",
    "href": "overview.html#differences-in-neighbours",
    "title": "overview",
    "section": "differences in neighbours",
    "text": "differences in neighbours\n\nnbs_vecs = dim_red_nbs_vecs(nbs_vecs, perplexity=70)\nnbs_diff = nbs_vecs.drop_duplicates(subset='lex', keep=False)\nnbs_diff = (nbs_diff\n    .groupby('subreddit')\n    .apply(lambda df: df.nlargest(20, 'sim'))\n    .reset_index(drop=True)\n)\n\n/var/folders/gp/dw55jb3d3gl6jn22rscvxjm40000gn/T/ipykernel_2126/2989563900.py:5: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  .apply(lambda df: df.nlargest(20, 'sim'))\n\n\n\nmap_diffs_plot = (alt.Chart(nbs_diff).mark_text().encode(\n        x='x_tsne:Q',\n        y='y_tsne:Q',\n        text='lex:N',\n        color='subreddit:N'\n    ))\n\n\nmap_diffs_plot",
    "crumbs": [
      "overview"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SocSemVar",
    "section": "",
    "text": "This repository contains the code for the paper Semantic change and socio-semantic variation. The case of Covid-related neologisms on Reddit, published in the journal Linguistics Vanguard.\nYou can clone the repository and install the code as a Python package named socsemvar by running pip install . within the cloned directory. This will automatically install all dependencies. As always, it is recommended to install this package in a virtual environment (e.g. using conda).\nThe Reddit data used for this paper are too big to make them available here. Therefore, the code in this repository uses a smaller random sample of the original data. The full datasets of Reddit comments and the models trained from these comments can be requested via email and allow to reproduce our results.\nThis repository provides the code used to process the Reddit comments, train the models, and produce the results presented in our paper. The code was written and documented using the literate programming framework nbdev and the documentation is available here:\nhttps://wuqui.github.io/socsemvar/",
    "crumbs": [
      "SocSemVar"
    ]
  },
  {
    "objectID": "embeddings.html",
    "href": "embeddings.html",
    "title": "embeddings",
    "section": "",
    "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload",
    "crumbs": [
      "embeddings"
    ]
  },
  {
    "objectID": "embeddings.html#train-models",
    "href": "embeddings.html#train-models",
    "title": "embeddings",
    "section": "train models",
    "text": "train models\n\ncreate corpus\n\n# fpaths = get_fpaths_year('2020')\nfpaths = get_fpaths_subreddit('conspiracy')\ncomments = read_multi_comments_csvs(fpaths)\ncomments_clean = clean_comments(comments)\n\nconv_to_lowerc       (2400, 5)  0:00:00.001485      \nrm_punct             (2400, 5)  0:00:00.014857      \ntokenize             (2400, 5)  0:00:00.003988      \nrem_short_comments   (1695, 5)  0:00:00.002041      \n\n\n\nclass Corpus:\n    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n    def __init__(self, docs):\n        self.docs_clean = docs\n\n    def __iter__(self):\n        for doc in self.docs_clean:\n            yield doc\n\n\nsource\n\n\nCorpus\n\n Corpus (docs)\n\nAn iterator that yields sentences (lists of str).\n\ncorpus = Corpus(comments_clean['body'].to_list())\n\n\n\ntrain model\n\ndef train_model(corpus,\n              MIN_COUNT=5,\n              SIZE=300,\n              WORKERS=8,\n              WINDOW=5,\n              EPOCHS=5\n              ):\n    model = Word2Vec(\n        corpus,\n        min_count=MIN_COUNT,\n        vector_size=SIZE,\n        workers=WORKERS,\n        window=WINDOW,\n        epochs=EPOCHS\n    )\n    return model\n\n\nsource\n\n\ntrain_model\n\n train_model (corpus, MIN_COUNT=5, SIZE=300, WORKERS=8, WINDOW=5,\n              EPOCHS=5)\n\n\nmodel = train_model(corpus)",
    "crumbs": [
      "embeddings"
    ]
  },
  {
    "objectID": "embeddings.html#load-models",
    "href": "embeddings.html#load-models",
    "title": "embeddings",
    "section": "load models",
    "text": "load models\n\ndef load_models(model_names: list, models_dir: str='../models_test') -&gt; dict:\n    models = {}\n    for name in model_names:\n        try:\n            models[name] = Word2Vec.load(f'{models_dir}/{name}.model')\n        except FileNotFoundError:\n            print(f\"Model '{name}' not found in '{models_dir}'.\")\n    return models\n\n\nsource\n\nload_models\n\n load_models (model_names:list, models_dir:str='../models_test')\n\n\nmodels = load_models(['2019', '2020', 'Coronavirus', 'conspiracy'])\nmodels\n\n{'2019': &lt;gensim.models.word2vec.Word2Vec&gt;,\n '2020': &lt;gensim.models.word2vec.Word2Vec&gt;,\n 'Coronavirus': &lt;gensim.models.word2vec.Word2Vec&gt;,\n 'conspiracy': &lt;gensim.models.word2vec.Word2Vec&gt;}",
    "crumbs": [
      "embeddings"
    ]
  },
  {
    "objectID": "embeddings.html#align-models",
    "href": "embeddings.html#align-models",
    "title": "embeddings",
    "section": "align models",
    "text": "align models\n\nassert len(models['2019'].wv.key_to_index) != len(models['2020'].wv.key_to_index)\n\n\ndef intersection_align_gensim(m1, m2, words=None):\n    \"\"\"\n    Intersect two gensim word2vec models, m1 and m2.\n    Only the shared vocabulary between them is kept.\n    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n        -- you can find the index of any word on the .index2word list: model.index2word.index(word) =&gt; 2\n    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n    \"\"\"\n\n    # Get the vocab for each model\n    vocab_m1 = set(m1.wv.index_to_key)\n    vocab_m2 = set(m2.wv.index_to_key)\n\n    # Find the common vocabulary\n    common_vocab = vocab_m1 & vocab_m2\n    if words: common_vocab &= set(words)\n\n    # If no alignment necessary because vocab is identical...\n    if not vocab_m1 - common_vocab and not vocab_m2 - common_vocab:\n        return (m1,m2)\n\n    # Otherwise sort by frequency (summed for both)\n    common_vocab = list(common_vocab)\n    common_vocab.sort(key=lambda w: m1.wv.get_vecattr(w, \"count\") + m2.wv.get_vecattr(w, \"count\"), reverse=True)\n    # print(len(common_vocab))\n\n    # Then for each model...\n    for m in [m1, m2]:\n        # Replace old syn0norm array with new one (with common vocab)\n        indices = [m.wv.key_to_index[w] for w in common_vocab]\n        old_arr = m.wv.vectors\n        new_arr = np.array([old_arr[index] for index in indices])\n        m.wv.vectors = new_arr\n\n        # Replace old vocab dictionary with new one (with common vocab)\n        # and old index2word with new one\n        new_key_to_index = {}\n        new_index_to_key = []\n        for new_index, key in enumerate(common_vocab):\n            new_key_to_index[key] = new_index\n            new_index_to_key.append(key)\n        m.wv.key_to_index = new_key_to_index\n        m.wv.index_to_key = new_index_to_key\n        \n        print(len(m.wv.key_to_index), len(m.wv.vectors))\n        \n    return (m1,m2)\n\n\nsource\n\nintersection_align_gensim\n\n intersection_align_gensim (m1, m2, words=None)\n\nIntersect two gensim word2vec models, m1 and m2. Only the shared vocabulary between them is kept. If ‘words’ is set (as list or set), then the vocabulary is intersected with this list as well. Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2). These indices correspond to the new syn0 and syn0norm objects in both gensim models: – so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0 – you can find the index of any word on the .index2word list: model.index2word.index(word) =&gt; 2 The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n\ndef smart_procrustes_align_gensim(base_embed, other_embed, words=None):\n    \"\"\"\n    Original script: https://gist.github.com/quadrismegistus/09a93e219a6ffc4f216fb85235535faf\n    Procrustes align two gensim word2vec models (to allow for comparison between same word across models).\n    Code ported from HistWords &lt;https://github.com/williamleif/histwords&gt; by William Hamilton &lt;wleif@stanford.edu&gt;.\n        \n    First, intersect the vocabularies (see `intersection_align_gensim` documentation).\n    Then do the alignment on the other_embed model.\n    Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.\n    Return other_embed.\n    If `words` is set, intersect the two models' vocabulary with the vocabulary in words (see `intersection_align_gensim` documentation).\n    \"\"\"\n\n    # make sure vocabulary and indices are aligned\n    in_base_embed, in_other_embed = intersection_align_gensim(base_embed, other_embed, words=words)\n\n    # get the (normalized) embedding matrices\n    base_vecs = in_base_embed.wv.get_normed_vectors()\n    other_vecs = in_other_embed.wv.get_normed_vectors()\n\n    # just a matrix dot product with numpy\n    m = other_vecs.T.dot(base_vecs) \n    # SVD method from numpy\n    u, _, v = np.linalg.svd(m)\n    # another matrix operation\n    ortho = u.dot(v) \n    # Replace original array with modified one, i.e. multiplying the embedding matrix by \"ortho\"\n    other_embed.wv.vectors = (other_embed.wv.vectors).dot(ortho)    \n    \n    return other_embed\n\n\nsource\n\n\nsmart_procrustes_align_gensim\n\n smart_procrustes_align_gensim (base_embed, other_embed, words=None)\n\nOriginal script: https://gist.github.com/quadrismegistus/09a93e219a6ffc4f216fb85235535faf Procrustes align two gensim word2vec models (to allow for comparison between same word across models). Code ported from HistWords https://github.com/williamleif/histwords by William Hamilton wleif@stanford.edu.\nFirst, intersect the vocabularies (see intersection_align_gensim documentation). Then do the alignment on the other_embed model. Replace the other_embed model’s syn0 and syn0norm numpy matrices with the aligned version. Return other_embed. If words is set, intersect the two models’ vocabulary with the vocabulary in words (see intersection_align_gensim documentation).\n\nsmart_procrustes_align_gensim(models['2019'], models['2020'])\n\n1710 1710\n1710 1710\n\n\n&lt;gensim.models.word2vec.Word2Vec&gt;\n\n\n\nassert len(models['2019'].wv.key_to_index) == len(models['2020'].wv.vectors)",
    "crumbs": [
      "embeddings"
    ]
  },
  {
    "objectID": "embeddings.html#measure-distances-between-types",
    "href": "embeddings.html#measure-distances-between-types",
    "title": "embeddings",
    "section": "measure distances between types",
    "text": "measure distances between types\n\ndef measure_distances(model_1, model_2):\n    distances = pd.DataFrame(\n        columns=('lex', 'dist_sem', \"freq_1\", \"freq_2\"),\n        data=(\n            #[w, spatial.distance.euclidean(model_1.wv[w], model_2.wv[w]),\n            #[w, np.sum(model_1.wv[w] * model_2.wv[w]) / (np.linalg.norm(model_1.wv[w]) * np.linalg.norm(model_2.wv[w])),\n            [w, spatial.distance.cosine(model_1.wv[w], model_2.wv[w]),\n             model_1.wv.get_vecattr(w, \"count\"),\n             model_2.wv.get_vecattr(w, \"count\")\n             ] for w in model_1.wv.index_to_key\n        )\n    )\n    return distances\n\n\nsource\n\nmeasure_distances\n\n measure_distances (model_1, model_2)\n\n\ndistances = measure_distances(models['2019'], models['2020'])\n\ndistances\\\n    .sort_values('dist_sem', ascending=False)\n\n\n\n\n\n\n\n\n\nlex\ndist_sem\nfreq_1\nfreq_2\n\n\n\n\n130\nbot\n0.179056\n106\n123\n\n\n147\naction\n0.172512\n98\n112\n\n\n1348\nforget\n0.171441\n8\n9\n\n\n62\nany\n0.155413\n243\n272\n\n\n94\nam\n0.153691\n158\n174\n\n\n...\n...\n...\n...\n...\n\n\n364\nposts\n0.000489\n32\n37\n\n\n300\nbig\n0.000456\n41\n47\n\n\n227\nlife\n0.000440\n58\n67\n\n\n242\nonce\n0.000392\n55\n62\n\n\n265\nanother\n0.000371\n48\n56\n\n\n\n\n1710 rows × 4 columns\n\n\n\n\n\nsource\n\n\nget_change_candidates\n\n get_change_candidates (k:int, distances:pandas.core.frame.DataFrame,\n                        freq_min:int=100, propNouns:bool=True)",
    "crumbs": [
      "embeddings"
    ]
  },
  {
    "objectID": "embeddings.html#get-nearest-semantic-neighbours",
    "href": "embeddings.html#get-nearest-semantic-neighbours",
    "title": "embeddings",
    "section": "get nearest semantic neighbours",
    "text": "get nearest semantic neighbours\n\ndef get_nearest_neighbours_models(lex: str, freq_min: int, model_1, model_2, topn: int=100_000, k: int=10):\n    nbs = []\n    for count, model in enumerate([model_1, model_2]):\n        for nb, sim in model.wv.most_similar(lex, topn=topn):\n            if model.wv.get_vecattr(nb, 'count') &gt; freq_min:\n                d = {}\n                d['Model'] = count + 1\n                d['Word'] = nb\n                d['SemDist'] = round(1 - sim, 2)\n                d['Freq'] = model.wv.get_vecattr(nb, \"count\")\n                d['vec'] = model.wv.get_vector(lex)\n                nbs.append(d)\n    nbs_df = pd.DataFrame(nbs)\n    nbs_df = nbs_df\\\n        .query('Freq &gt; @freq_min')\\\n        .groupby('Model', group_keys=False)\\\n        .apply(lambda group: group.nsmallest(k, 'SemDist'))\n    nbs_model_1 = nbs_df.query('Model == 1')\n    nbs_model_2 = nbs_df.query('Model == 2')\n    return nbs_model_1, nbs_model_2\n\n\nsource\n\nget_nearest_neighbours_models\n\n get_nearest_neighbours_models (lex:str, freq_min:int, model_1, model_2,\n                                topn:int=100000, k:int=10)\n\n\nnbs_1, nbs_2 = get_nearest_neighbours_models('good', 5, models['2019'], models['2020'])\n\n/var/folders/gp/dw55jb3d3gl6jn22rscvxjm40000gn/T/ipykernel_48107/1648447668.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  .apply(lambda group: group.nsmallest(k, 'SemDist'))\n\n\n\nnbs_1\n\n\n\n\n\n\n\n\n\nModel\nWord\nSemDist\nFreq\nvec\n\n\n\n\n0\n1\nher\n0.0\n166\n[-0.00916857, 0.2121922, -0.025869183, 0.07142...\n\n\n1\n1\nme\n0.0\n332\n[-0.00916857, 0.2121922, -0.025869183, 0.07142...\n\n\n2\n1\nmuch\n0.0\n150\n[-0.00916857, 0.2121922, -0.025869183, 0.07142...\n\n\n3\n1\nas\n0.0\n487\n[-0.00916857, 0.2121922, -0.025869183, 0.07142...\n\n\n4\n1\npeople\n0.0\n293\n[-0.00916857, 0.2121922, -0.025869183, 0.07142...\n\n\n5\n1\none\n0.0\n280\n[-0.00916857, 0.2121922, -0.025869183, 0.07142...\n\n\n6\n1\nhad\n0.0\n178\n[-0.00916857, 0.2121922, -0.025869183, 0.07142...\n\n\n7\n1\nnow\n0.0\n150\n[-0.00916857, 0.2121922, -0.025869183, 0.07142...\n\n\n8\n1\nall\n0.0\n374\n[-0.00916857, 0.2121922, -0.025869183, 0.07142...\n\n\n9\n1\nhim\n0.0\n146\n[-0.00916857, 0.2121922, -0.025869183, 0.07142...\n\n\n\n\n\n\n\n\n\nnbs_2\n\n\n\n\n\n\n\n\n\nModel\nWord\nSemDist\nFreq\nvec\n\n\n\n\n1709\n2\nwell\n0.0\n144\n[-0.0038482083, 0.21741627, -0.03928126, 0.050...\n\n\n1710\n2\nalso\n0.0\n184\n[-0.0038482083, 0.21741627, -0.03928126, 0.050...\n\n\n1711\n2\nme\n0.0\n371\n[-0.0038482083, 0.21741627, -0.03928126, 0.050...\n\n\n1712\n2\nve\n0.0\n178\n[-0.0038482083, 0.21741627, -0.03928126, 0.050...\n\n\n1713\n2\nmeans\n0.0\n33\n[-0.0038482083, 0.21741627, -0.03928126, 0.050...\n\n\n1714\n2\nmuch\n0.0\n167\n[-0.0038482083, 0.21741627, -0.03928126, 0.050...\n\n\n1715\n2\nyeah\n0.0\n81\n[-0.0038482083, 0.21741627, -0.03928126, 0.050...\n\n\n1716\n2\nidea\n0.0\n41\n[-0.0038482083, 0.21741627, -0.03928126, 0.050...\n\n\n1717\n2\nsure\n0.0\n114\n[-0.0038482083, 0.21741627, -0.03928126, 0.050...\n\n\n1718\n2\nwhen\n0.0\n296\n[-0.0038482083, 0.21741627, -0.03928126, 0.050...",
    "crumbs": [
      "embeddings"
    ]
  },
  {
    "objectID": "embeddings.html#semantic-axes",
    "href": "embeddings.html#semantic-axes",
    "title": "embeddings",
    "section": "semantic axes",
    "text": "semantic axes\n\nmodels = load_models(['Coronavirus', 'conspiracy'])\n\n\ndef get_pole_avg(model_name: str, model: Word2Vec, lex, k=10):\n    words = []\n    vecs = []\n    vecs.append(model.wv[lex])\n    df = (pd.read_csv(f\"../pole-words/{model_name}_{lex}.csv\")\n        .query('Include != \"f\"')\n        .nlargest(k, 'SemSim')\n    )\n    pole_words = df['Word'].tolist()\n    for word in pole_words:\n        if word in model.wv:\n            vecs.append(model.wv[word])\n            words.append(word)\n    pole_avg = np.mean(vecs, axis=0)\n    return pole_avg\n\n\nsource\n\nget_pole_avg\n\n get_pole_avg (model_name:str, model:gensim.models.word2vec.Word2Vec, lex,\n               k=10)\n\n\nget_pole_avg('Coronavirus', models['Coronavirus'], 'good').shape\n\n(300,)\n\n\n\ndef make_sem_axis_avg(model_name: str, model: Word2Vec, pole_word_1: str, pole_word_2: str, k=10):\n    pole_1_avg = get_pole_avg(model_name, model, pole_word_1, k)\n    pole_2_avg = get_pole_avg(model_name, model, pole_word_2, k)\n    sem_axis = pole_1_avg - pole_2_avg\n    return sem_axis\n\n\nsource\n\n\nmake_sem_axis_avg\n\n make_sem_axis_avg (model_name:str, model:gensim.models.word2vec.Word2Vec,\n                    pole_word_1:str, pole_word_2:str, k=10)\n\n\nmake_sem_axis_avg('Coronavirus', models['Coronavirus'], 'good', 'bad').shape\n\n(300,)\n\n\n\ndef get_axis_sim(lex: str, pole_word_1: str, pole_word_2: str, model_name, model, k=10):\n    sem_axis = make_sem_axis_avg(model_name, model, pole_word_1, pole_word_2, k)\n    lex_vec = model.wv.get_vector(lex)\n    sim_cos = 1 - spatial.distance.cosine(lex_vec, sem_axis)\n    return sim_cos\n\n\nsource\n\n\nget_axis_sim\n\n get_axis_sim (lex:str, pole_word_1:str, pole_word_2:str, model_name,\n               model, k=10)\n\n\nget_axis_sim('vaccines', 'good', 'bad', 'Coronavirus', models['Coronavirus'], k=10)\n\n0.9172929190814479\n\n\n\ndef get_axis_sims(lexs: list, models: dict, pole_words: list, k=10):\n    sims = []\n    for lex in lexs:\n        for name, model in models.items():\n            sim = {}\n            sim['model'] = name\n            sim['lex'] = lex\n            sim['sim'] = get_axis_sim(lex, pole_words[0], pole_words[1], name, model, k)\n            sims.append(sim)\n    sims_df = pd.DataFrame(sims)\n    return sims_df\n\n\nsource\n\n\nget_axis_sims\n\n get_axis_sims (lexs:list, models:dict, pole_words:list, k=10)\n\n\nproj_sims = get_axis_sims(['vaccines', 'vaccine'], models, ['good', 'bad'])\nproj_sims\n\n\n\n\n\n\n\n\n\nmodel\nlex\nsim\n\n\n\n\n0\nCoronavirus\nvaccines\n0.917293\n\n\n1\nconspiracy\nvaccines\n0.990792\n\n\n2\nCoronavirus\nvaccine\n0.933222\n\n\n3\nconspiracy\nvaccine\n0.977039\n\n\n\n\n\n\n\n\n\nsource\n\n\naggregate_proj_sims\n\n aggregate_proj_sims (proj_sims)\n\n\nproj_sims = aggregate_proj_sims(proj_sims)\nproj_sims\n\n\n\n\n\n\n\n\nmodel\nlex\nCoronavirus\nconspiracy\nSimDiff\n\n\n\n\n0\nvaccine\n0.933222\n0.977039\n-0.043817\n\n\n1\nvaccines\n0.917293\n0.990792\n-0.073499\n\n\n\n\n\n\n\n\n\nproj_sims_melted = proj_sims.melt(id_vars=['lex', 'SimDiff'], var_name='model', value_name='SemSim')\nproj_sims_melted\n\n\n\n\n\n\n\n\n\nlex\nSimDiff\nmodel\nSemSim\n\n\n\n\n0\nvaccine\n-0.043817\nCoronavirus\n0.933222\n\n\n1\nvaccines\n-0.073499\nCoronavirus\n0.917293\n\n\n2\nvaccine\n-0.043817\nconspiracy\n0.977039\n\n\n3\nvaccines\n-0.073499\nconspiracy\n0.990792\n\n\n\n\n\n\n\n\n\nsource\n\n\nplot_sem_axis\n\n plot_sem_axis (proj_sims_melted:pandas.core.frame.DataFrame,\n                pole_words:list)\n\n\nplot_sem_axis(proj_sims_melted, ['good', 'bad'])",
    "crumbs": [
      "embeddings"
    ]
  },
  {
    "objectID": "embeddings.html#maps-of-socio-semantic-variation",
    "href": "embeddings.html#maps-of-socio-semantic-variation",
    "title": "embeddings",
    "section": "maps of socio-semantic variation",
    "text": "maps of socio-semantic variation\n\nmodels = load_models(['Coronavirus', 'conspiracy'])\n\n\nsmart_procrustes_align_gensim(models['Coronavirus'], models['conspiracy'])\n\n1173 1173\n1173 1173\n\n\n&lt;gensim.models.word2vec.Word2Vec&gt;\n\n\n\ndef get_nbs_vecs(lex: str, model_name: str, model: Word2Vec, k=50):\n    lex_vecs = []\n    lex_d = {}\n    lex_d['lex'] = lex\n    lex_d['type'] = 'center'\n    lex_d['subreddit'] = model_name\n    lex_d['vec'] = model.wv.get_vector(lex)\n    lex_vecs.append(lex_d)\n    for nb, sim in model.wv.most_similar(lex, topn=k):\n        lex_d = {}\n        lex_d['lex'] = nb\n        lex_d['type'] = 'nb'\n        lex_d['sim'] = sim\n        lex_d['subreddit'] = model_name\n        lex_d['vec'] =  model.wv.get_vector(nb)\n        lex_d['freq'] = model.wv.get_vecattr(nb, \"count\")\n        lex_vecs.append(lex_d)\n    lex_vecs_df = pd.DataFrame(lex_vecs)\n    return lex_vecs_df\n\n\nsource\n\nget_nbs_vecs\n\n get_nbs_vecs (lex:str, model_name:str,\n               model:gensim.models.word2vec.Word2Vec, k=50)\n\n\nget_nbs_vecs('vaccines', 'Coronavirus', models['Coronavirus'])\n\n\n\n\n\n\n\n\n\nlex\ntype\nsubreddit\nvec\nsim\nfreq\n\n\n\n\n0\nvaccines\ncenter\nCoronavirus\n[0.057227388, 0.10430427, 0.032422945, 0.06407...\nNaN\nNaN\n\n\n1\nhealth\nnb\nCoronavirus\n[0.114723645, 0.21284297, 0.064497545, 0.12715...\n0.999692\n46.0\n\n\n2\nper\nnb\nCoronavirus\n[0.090133496, 0.16385178, 0.045519162, 0.09794...\n0.999669\n29.0\n\n\n3\ndeaths\nnb\nCoronavirus\n[0.11426731, 0.20661092, 0.06342163, 0.1284676...\n0.999660\n49.0\n\n\n4\n1\nnb\nCoronavirus\n[0.11298724, 0.20978343, 0.061724722, 0.128311...\n0.999638\n54.0\n\n\n5\nus\nnb\nCoronavirus\n[0.11077853, 0.21411182, 0.06554103, 0.1271514...\n0.999608\n123.0\n\n\n6\nnew\nnb\nCoronavirus\n[0.1211073, 0.22653188, 0.069263056, 0.1391144...\n0.999603\n76.0\n\n\n7\nyears\nnb\nCoronavirus\n[0.09558547, 0.18013933, 0.05468028, 0.1110726...\n0.999591\n52.0\n\n\n8\nspread\nnb\nCoronavirus\n[0.07730215, 0.14945872, 0.04962611, 0.0888780...\n0.999588\n35.0\n\n\n9\n0\nnb\nCoronavirus\n[0.0645965, 0.1220492, 0.035898782, 0.07554879...\n0.999573\n16.0\n\n\n10\n000\nnb\nCoronavirus\n[0.075567916, 0.1435639, 0.040953316, 0.084523...\n0.999569\n29.0\n\n\n11\n7\nnb\nCoronavirus\n[0.06406919, 0.12866612, 0.03618963, 0.0769648...\n0.999565\n17.0\n\n\n12\nworld\nnb\nCoronavirus\n[0.08870022, 0.159735, 0.04859646, 0.09668462,...\n0.999564\n74.0\n\n\n13\nanother\nnb\nCoronavirus\n[0.06684075, 0.12619431, 0.037108395, 0.077369...\n0.999556\n42.0\n\n\n14\npublic\nnb\nCoronavirus\n[0.08594237, 0.16238208, 0.054486435, 0.097496...\n0.999552\n39.0\n\n\n15\nhours\nnb\nCoronavirus\n[0.055532847, 0.11181068, 0.032346416, 0.06532...\n0.999545\n15.0\n\n\n16\ncontrol\nnb\nCoronavirus\n[0.0560417, 0.10506418, 0.030902166, 0.0663534...\n0.999544\n31.0\n\n\n17\nkeep\nnb\nCoronavirus\n[0.08326553, 0.15533412, 0.04887768, 0.0957071...\n0.999539\n85.0\n\n\n18\nreported\nnb\nCoronavirus\n[0.07306963, 0.1333254, 0.045092944, 0.0796391...\n0.999537\n19.0\n\n\n19\naverage\nnb\nCoronavirus\n[0.058386106, 0.10160504, 0.028476257, 0.06527...\n0.999528\n17.0\n\n\n20\nsars\nnb\nCoronavirus\n[0.054962628, 0.10801497, 0.030391451, 0.06244...\n0.999521\n14.0\n\n\n21\nago\nnb\nCoronavirus\n[0.05723345, 0.1124895, 0.03595097, 0.06608445...\n0.999521\n29.0\n\n\n22\nscience\nnb\nCoronavirus\n[0.06259699, 0.11655103, 0.03798895, 0.0715784...\n0.999516\n24.0\n\n\n23\nunder\nnb\nCoronavirus\n[0.08245373, 0.15575409, 0.054234445, 0.097904...\n0.999508\n40.0\n\n\n24\nearly\nnb\nCoronavirus\n[0.07038852, 0.12683387, 0.044630148, 0.081177...\n0.999506\n19.0\n\n\n25\nlikely\nnb\nCoronavirus\n[0.06962872, 0.13197441, 0.043262962, 0.075477...\n0.999506\n36.0\n\n\n26\ndue\nnb\nCoronavirus\n[0.052545387, 0.09022615, 0.028288025, 0.05750...\n0.999492\n25.0\n\n\n27\nschools\nnb\nCoronavirus\n[0.07024904, 0.13720992, 0.042491872, 0.087135...\n0.999490\n22.0\n\n\n28\nstate\nnb\nCoronavirus\n[0.08707496, 0.16359739, 0.054630436, 0.101618...\n0.999488\n47.0\n\n\n29\nquality\nnb\nCoronavirus\n[0.05661944, 0.10371776, 0.03314033, 0.0622458...\n0.999486\n14.0\n\n\n30\n4\nnb\nCoronavirus\n[0.057619605, 0.117240496, 0.036980513, 0.0707...\n0.999483\n26.0\n\n\n31\nhelp\nnb\nCoronavirus\n[0.074597746, 0.13663343, 0.040477894, 0.08401...\n0.999481\n34.0\n\n\n32\navailable\nnb\nCoronavirus\n[0.050409965, 0.097474575, 0.028630486, 0.0579...\n0.999480\n13.0\n\n\n33\namerica\nnb\nCoronavirus\n[0.055805784, 0.09872247, 0.029724134, 0.06394...\n0.999477\n30.0\n\n\n34\nresponse\nnb\nCoronavirus\n[0.04918535, 0.08948149, 0.022905065, 0.054223...\n0.999472\n12.0\n\n\n35\nhigh\nnb\nCoronavirus\n[0.10157195, 0.19419962, 0.06280887, 0.1199198...\n0.999469\n39.0\n\n\n36\n10\nnb\nCoronavirus\n[0.08214546, 0.14870152, 0.04579945, 0.0888806...\n0.999461\n32.0\n\n\n37\nam\nnb\nCoronavirus\n[0.096367285, 0.18651997, 0.044814415, 0.11034...\n0.999456\n102.0\n\n\n38\ncity\nnb\nCoronavirus\n[0.06082973, 0.11749765, 0.034887813, 0.073830...\n0.999453\n19.0\n\n\n39\nmonths\nnb\nCoronavirus\n[0.09699332, 0.17284966, 0.057289552, 0.105755...\n0.999451\n44.0\n\n\n40\ncurrently\nnb\nCoronavirus\n[0.062632866, 0.12303838, 0.03613432, 0.074787...\n0.999444\n19.0\n\n\n41\nalready\nnb\nCoronavirus\n[0.09850482, 0.1830824, 0.059671603, 0.1124184...\n0.999443\n49.0\n\n\n42\nfact\nnb\nCoronavirus\n[0.063221715, 0.123731576, 0.04299859, 0.07519...\n0.999428\n44.0\n\n\n43\nplaces\nnb\nCoronavirus\n[0.058041047, 0.11331363, 0.038818423, 0.06794...\n0.999427\n18.0\n\n\n44\nprovide\nnb\nCoronavirus\n[0.04588933, 0.095273994, 0.023019709, 0.05869...\n0.999418\n14.0\n\n\n45\npositive\nnb\nCoronavirus\n[0.067284316, 0.12324461, 0.040740743, 0.07202...\n0.999414\n19.0\n\n\n46\nlink\nnb\nCoronavirus\n[0.04832779, 0.08898213, 0.022611374, 0.052865...\n0.999399\n27.0\n\n\n47\nbased\nnb\nCoronavirus\n[0.073736355, 0.14148337, 0.03525661, 0.082236...\n0.999397\n24.0\n\n\n48\n20\nnb\nCoronavirus\n[0.09155286, 0.18733306, 0.051601738, 0.109539...\n0.999391\n28.0\n\n\n49\nthan\nnb\nCoronavirus\n[0.14099781, 0.26505557, 0.09407188, 0.1668624...\n0.999377\n124.0\n\n\n50\nopen\nnb\nCoronavirus\n[0.08959826, 0.16488104, 0.051867336, 0.098772...\n0.999374\n36.0\n\n\n\n\n\n\n\n\n\nnbs_vecs = pd.concat([get_nbs_vecs('vaccines', model_name, model, k=750) for model_name, model in models.items()])\n\n\nnbs_vecs['vec'].iloc[0].shape\n\n(300,)\n\n\n\ndef dim_red_nbs_vecs(nbs_vecs: pd.DataFrame, perplexity=50, n_iter=1000):\n    Y_tsne = TSNE(\n            perplexity=perplexity,\n            method='exact',\n            init='pca',\n            verbose=False,\n            learning_rate='auto',\n            n_iter=n_iter\n        ).fit_transform(np.array(list(nbs_vecs['vec'])))\n    nbs_vecs['x_tsne'] = Y_tsne[:, [0]]\n    nbs_vecs['y_tsne'] = Y_tsne[:, [1]]\n    return nbs_vecs\n\n\nsource\n\n\ndim_red_nbs_vecs\n\n dim_red_nbs_vecs (nbs_vecs:pandas.core.frame.DataFrame, perplexity=50,\n                   n_iter=1000)\n\n\nnbs_vecs_dimred = dim_red_nbs_vecs(nbs_vecs, n_iter=250)\n\n\nprint(nbs_vecs_dimred['x_tsne'].iloc[0].shape, nbs_vecs_dimred['y_tsne'].iloc[0].shape)\n\n() ()\n\n\n\n\ncommon neighbours\n\nnbs_vecs = dim_red_nbs_vecs(nbs_vecs, perplexity=0.1, n_iter=250)\n\n\nnbs_sim = (nbs_vecs\n    .groupby('subreddit')\n    .apply(lambda df: df.nlargest(10, 'sim'))\n    .reset_index(drop=True)\n)\n\n/var/folders/gp/dw55jb3d3gl6jn22rscvxjm40000gn/T/ipykernel_48107/1291324649.py:3: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  .apply(lambda df: df.nlargest(10, 'sim'))\n\n\n\nchart_sims = (alt.Chart(nbs_sim).mark_text().encode(\n        x='x_tsne:Q',\n        y='y_tsne:Q',\n        text='lex',\n        color='subreddit:N'\n    ))\n\nchart_sims\n\n\n\n\n\n\n\n\n\ndifferences in neighbours\n\nnbs_vecs = dim_red_nbs_vecs(nbs_vecs, perplexity=70, n_iter=250)\n\n\nnbs_diff = nbs_vecs.drop_duplicates(subset='lex', keep=False)\nnbs_diff = (nbs_diff\n    .groupby('subreddit')\n    .apply(lambda df: df.nlargest(20, 'sim'))\n    .reset_index(drop=True)\n)\n\n/var/folders/gp/dw55jb3d3gl6jn22rscvxjm40000gn/T/ipykernel_48107/628690727.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  .apply(lambda df: df.nlargest(20, 'sim'))\n\n\n\nchart_diffs = (alt.Chart(nbs_diff).mark_text().encode(\n        x='x_tsne:Q',\n        y='y_tsne:Q',\n        text='lex:N',\n        color='subreddit:N'\n    ))\n\n\nchart_diffs",
    "crumbs": [
      "embeddings"
    ]
  },
  {
    "objectID": "read_data.html",
    "href": "read_data.html",
    "title": "read data",
    "section": "",
    "text": "This repository only contains a sample of the original data because of size constraints. Below, we create a sample of the original data.\n\nfrom tqdm.notebook import tqdm\n\nsrc_dir = Path('../../data')\ndest_dir = Path('../data_test')\n\nn = 100\n\nfor subdir in src_dir.rglob('*'):\n    if subdir.is_dir():\n        dest_subdir = dest_dir / subdir.relative_to(src_dir)\n        dest_subdir.mkdir(parents=True, exist_ok=True)\n        for file_path in tqdm(list(subdir.glob('*.csv'))):\n            df = pd.read_csv(file_path, on_bad_lines='skip', engine='python')\n            df_sample = df.sample(n=min(n, len(df)), random_state=58)\n            sample_file_path = dest_subdir / file_path.name\n            df_sample.to_csv(sample_file_path, index=False)",
    "crumbs": [
      "read data"
    ]
  },
  {
    "objectID": "read_data.html#create-test-data",
    "href": "read_data.html#create-test-data",
    "title": "read data",
    "section": "",
    "text": "This repository only contains a sample of the original data because of size constraints. Below, we create a sample of the original data.\n\nfrom tqdm.notebook import tqdm\n\nsrc_dir = Path('../../data')\ndest_dir = Path('../data_test')\n\nn = 100\n\nfor subdir in src_dir.rglob('*'):\n    if subdir.is_dir():\n        dest_subdir = dest_dir / subdir.relative_to(src_dir)\n        dest_subdir.mkdir(parents=True, exist_ok=True)\n        for file_path in tqdm(list(subdir.glob('*.csv'))):\n            df = pd.read_csv(file_path, on_bad_lines='skip', engine='python')\n            df_sample = df.sample(n=min(n, len(df)), random_state=58)\n            sample_file_path = dest_subdir / file_path.name\n            df_sample.to_csv(sample_file_path, index=False)",
    "crumbs": [
      "read data"
    ]
  },
  {
    "objectID": "read_data.html#read-data",
    "href": "read_data.html#read-data",
    "title": "read data",
    "section": "read data",
    "text": "read data\n\nget file paths\n\ndef get_fpaths_year(year: str,\n            dir='../data_test/years'\n            ) -&gt; list: \n    dir_path = Path(dir)\n    return list(dir_path.glob(f'{year}*.csv'))\n\n\nsource\n\n\nget_fpaths_year\n\n get_fpaths_year (year:str, dir='../data_test/years')\n\n\nget_fpaths_year('2020')\n\n[PosixPath('../data_test/years/2020-04-14_21:20:57___2020-04-14_21:59:59.csv'),\n PosixPath('../data_test/years/2020-04-07_21:19:06___2020-04-07_21:59:59.csv'),\n PosixPath('../data_test/years/2020-06-14_21:19:36___2020-06-14_21:59:59.csv'),\n PosixPath('../data_test/years/2020-10-14_21:19:48___2020-10-14_21:59:59.csv'),\n PosixPath('../data_test/years/2020-06-07_21:19:08___2020-06-07_21:59:59.csv'),\n PosixPath('../data_test/years/2020-02-07_22:18:35___2020-02-07_22:59:59.csv'),\n PosixPath('../data_test/years/2020-07-14_21:22:47___2020-07-14_21:59:59.csv'),\n PosixPath('../data_test/years/2020-06-01_21:59:59___2020-06-01_21:22:24.csv'),\n PosixPath('../data_test/years/2020-09-19_21:14:30___2020-09-19_21:59:59.csv'),\n PosixPath('../data_test/years/2020-07-01_21:59:59___2020-07-01_21:23:38.csv'),\n PosixPath('../data_test/years/2020-10-19_14:18:54___2020-10-19_14:58:31.csv'),\n PosixPath('../data_test/years/2020-02-01_22:59:59___2020-02-01_22:07:59.csv'),\n PosixPath('../data_test/years/2020-08-14_21:23:04___2020-08-14_21:59:59.csv'),\n PosixPath('../data_test/years/2020-03-19_22:14:44___2020-03-19_22:59:59.csv'),\n PosixPath('../data_test/years/2020-11-01_22:59:59___2020-11-01_22:20:51.csv'),\n PosixPath('../data_test/years/2020-01-01_22:59:59___2020-01-01_22:13:27.csv'),\n PosixPath('../data_test/years/2020-07-19_21:19:00___2020-07-19_21:59:59.csv'),\n PosixPath('../data_test/years/2020-10-07_21:20:42___2020-10-07_21:59:59.csv'),\n PosixPath('../data_test/years/2020-08-01_21:59:59___2020-08-01_21:17:27.csv'),\n PosixPath('../data_test/years/2020-03-07_22:37:38___2020-03-07_22:59:59.csv'),\n PosixPath('../data_test/years/2020-05-14_21:22:43___2020-05-14_21:59:59.csv'),\n PosixPath('../data_test/years/2020-12-07_22:21:23___2020-12-07_22:59:59.csv'),\n PosixPath('../data_test/years/2020-03-14_22:09:26___2020-03-14_22:59:59.csv'),\n PosixPath('../data_test/years/2020-05-01_21:59:59___2020-05-01_21:19:50.csv'),\n PosixPath('../data_test/years/2020-01-19_22:39:11___2020-01-19_22:59:59.csv'),\n PosixPath('../data_test/years/2020-08-07_21:22:06___2020-08-07_21:59:59.csv'),\n PosixPath('../data_test/years/2020-09-07_21:19:15___2020-09-07_21:59:59.csv'),\n PosixPath('../data_test/years/2020-07-07_21:22:39___2020-07-07_21:59:59.csv'),\n PosixPath('../data_test/years/2020-04-19_21:17:49___2020-04-19_21:59:59.csv'),\n PosixPath('../data_test/years/2020-01-14_22:13:57___2020-01-14_22:59:59.csv'),\n PosixPath('../data_test/years/2020-11-19_22:27:49___2020-11-19_22:59:59.csv'),\n PosixPath('../data_test/years/2020-12-01_22:59:59___2020-12-01_22:22:27.csv'),\n PosixPath('../data_test/years/2020-05-07_21:22:03___2020-05-07_21:59:59.csv'),\n PosixPath('../data_test/years/2020-01-07_22:21:30___2020-01-07_22:59:59.csv'),\n PosixPath('../data_test/years/2020-11-07_22:19:48___2020-11-07_22:59:59.csv'),\n PosixPath('../data_test/years/2020-11-14_22:15:34___2020-11-14_22:59:59.csv'),\n PosixPath('../data_test/years/2020-09-14_21:31:19___2020-09-14_21:59:59.csv'),\n PosixPath('../data_test/years/2020-05-19_21:22:04___2020-05-19_21:59:59.csv'),\n PosixPath('../data_test/years/2020-09-01_21:59:59___2020-09-01_21:24:54.csv'),\n PosixPath('../data_test/years/2020-10-01_21:59:59___2020-10-01_21:20:26.csv'),\n PosixPath('../data_test/years/2020-12-19_22:29:40___2020-12-19_22:59:59.csv'),\n PosixPath('../data_test/years/2020-03-01_22:59:59___2020-03-01_22:11:57.csv'),\n PosixPath('../data_test/years/2020-04-01_21:59:59___2020-04-01_21:18:25.csv'),\n PosixPath('../data_test/years/2020-12-14_22:21:47___2020-12-14_22:59:59.csv'),\n PosixPath('../data_test/years/2020-02-14_22:23:09___2020-02-14_22:59:59.csv'),\n PosixPath('../data_test/years/2020-02-19_22:14:28___2020-02-19_22:59:59.csv'),\n PosixPath('../data_test/years/2020-06-19_21:18:29___2020-06-19_21:59:59.csv'),\n PosixPath('../data_test/years/2020-08-19_21:22:40___2020-08-19_21:59:59.csv')]\n\n\n\ndef get_fpaths_subreddit(subreddit: str, dir: str='../data_test/subreddits') -&gt; list: \n    dir_path = Path(dir)\n    return list(dir_path.glob(f'{subreddit}*.csv'))\n\n\nsource\n\n\nget_fpaths_subreddit\n\n get_fpaths_subreddit (subreddit:str, dir:str='../data_test/subreddits')\n\n\nget_fpaths_subreddit('conspiracy')\n\n[PosixPath('../data_test/subreddits/conspiracy___2020-11-17_11:02:26___2020-11-27_22:59:54.csv'),\n PosixPath('../data_test/subreddits/conspiracy___2020-03-01_23:00:02___2020-03-09_22:59:59.csv'),\n PosixPath('../data_test/subreddits/conspiracy___2020-01-03_19:39:57___2020-01-27_22:59:58.csv'),\n PosixPath('../data_test/subreddits/conspiracy___2020-04-17_04:25:29___2020-04-27_21:59:55.csv'),\n PosixPath('../data_test/subreddits/conspiracy___2020-05-14_00:35:50___2020-05-27_21:59:58.csv'),\n PosixPath('../data_test/subreddits/conspiracy___2020-02-06_03:54:59___2020-02-27_22:59:57.csv'),\n PosixPath('../data_test/subreddits/conspiracy___2020-07-01_22:00:04___2020-07-09_21:59:58.csv'),\n PosixPath('../data_test/subreddits/conspiracy___2020-11-01_23:00:04___2020-11-09_22:59:56.csv'),\n PosixPath('../data_test/subreddits/conspiracy___2020-02-01_23:00:04___2020-02-09_22:59:55.csv'),\n PosixPath('../data_test/subreddits/conspiracy___2020-08-01_22:00:01___2020-08-09_21:59:56.csv'),\n PosixPath('../data_test/subreddits/conspiracy___2020-10-16_22:19:53___2020-10-27_22:59:46.csv'),\n PosixPath('../data_test/subreddits/conspiracy___2020-09-01_22:00:03___2020-09-09_21:59:56.csv'),\n PosixPath('../data_test/subreddits/conspiracy___2020-03-17_15:00:57___2020-03-27_22:59:52.csv'),\n PosixPath('../data_test/subreddits/conspiracy___2020-05-01_22:00:03___2020-05-09_21:59:48.csv'),\n PosixPath('../data_test/subreddits/conspiracy___2020-06-15_13:59:56___2020-06-27_21:59:55.csv'),\n PosixPath('../data_test/subreddits/conspiracy___2020-07-18_14:50:04___2020-07-27_21:59:59.csv'),\n PosixPath('../data_test/subreddits/conspiracy___2020-12-15_22:12:50___2020-12-27_22:59:52.csv'),\n PosixPath('../data_test/subreddits/conspiracy___2020-04-01_22:00:01___2020-04-09_21:59:49.csv'),\n PosixPath('../data_test/subreddits/conspiracy___2020-09-13_14:27:06___2020-09-27_21:59:59.csv'),\n PosixPath('../data_test/subreddits/conspiracy___2020-12-01_23:00:02___2020-12-09_22:59:59.csv'),\n PosixPath('../data_test/subreddits/conspiracy___2020-06-01_22:00:04___2020-06-09_21:59:54.csv'),\n PosixPath('../data_test/subreddits/conspiracy___2020-01-01_23:00:03___2020-01-09_22:59:54.csv'),\n PosixPath('../data_test/subreddits/conspiracy___2020-10-01_22:00:10___2020-10-09_21:59:57.csv'),\n PosixPath('../data_test/subreddits/conspiracy___2020-08-17_21:01:44___2020-08-27_21:59:57.csv')]\n\n\n\n\nread comments\n\nread a single csv file of comments\n\nfpath = get_fpaths_year('2019')[0]\n\n\ndef read_one_comments_csv(fpath: str) -&gt; pd.DataFrame:\n    try:\n        comments = pd.read_csv(\n            fpath,\n            usecols=['id', 'created_utc', 'author', 'subreddit', 'body'],\n            dtype={\n                'id': 'string',\n                'author': 'string',\n                'subreddit': 'string',\n                'body': 'string'\n            },\n            parse_dates=['created_utc'],\n            low_memory=False,\n            lineterminator='\\n'\n        )\n        comments_clean = comments\\\n            .dropna()\\\n            .drop_duplicates(subset='id')\n        return comments_clean\n    except FileNotFoundError:\n        print(f'{fpath} not found on disk')\n    except pd.errors.EmptyDataError:\n        print(f'{fpath} is empty')\n\n\nsource\n\n\n\nread_one_comments_csv\n\n read_one_comments_csv (fpath:str)\n\n\nread_one_comments_csv(fpath)\n\n\n\n\n\n\n\n\n\nauthor\nbody\ncreated_utc\nid\nsubreddit\n\n\n\n\n0\nlilfooty\nThis'll hurt them more than the loss\n2019-05-07 21:55:57\nemrz5jp\nsoccer\n\n\n1\nKaeneko\nI loved vampires *so* much, lol. Always fantas...\n2019-05-07 21:34:12\nemrx5eq\nBDSMcommunity\n\n\n2\nLes_Deplorables\nPoor Zombies gonna starve. No Brains!\n2019-05-07 21:21:11\nemrvxjq\nThe_Donald\n\n\n3\nviper2544\nNo one is going to mention the $12 shipping?\n2019-05-07 21:56:45\nemrz8g7\nlegostarwars\n\n\n4\nninjasquirrelarmy\nAgreed. I showed my stylist the Phoenix hair ...\n2019-05-07 21:34:43\nemrx730\nInstagramreality\n\n\n...\n...\n...\n...\n...\n...\n\n\n95\nkleptominotaur\nIs tj still a muscle pharm dude? or parm? what...\n2019-05-07 21:35:05\nemrx88m\nMMA\n\n\n96\nbonesstackedonfloor\nFidgeting\n2019-05-07 21:45:27\nemry5y5\nAskReddit\n\n\n97\nPerfectoi\nImagine thinking EV will be sacked. Friendly r...\n2019-05-07 21:18:06\nemrvmx0\nBarca\n\n\n98\nBB-Zwei\nAnd Dumbo.\n2019-05-07 21:51:35\nemryq8t\nmovies\n\n\n99\nThomas2PP\nHow did he got it???\n2019-05-07 21:15:42\nemrveqf\nhearthstone\n\n\n\n\n100 rows × 5 columns\n\n\n\n\n\nread multiple csv files with comments\n\ndef read_multi_comments_csvs(fpaths: list) -&gt; pd.DataFrame:\n    comments_lst = []\n    for fpath in fpaths:\n        comments = read_one_comments_csv(fpath)\n        comments_lst.append(comments)\n    comments_concat = pd.concat(\n        comments_lst,\n        axis=0,\n        ignore_index=True\n    )\n    return comments_concat\n\n\nsource\n\n\n\nread_multi_comments_csvs\n\n read_multi_comments_csvs (fpaths:list)\n\n\nfpaths = get_fpaths_year('2019')\n\n\nread_multi_comments_csvs(fpaths)\n\n\n\n\n\n\n\n\n\nauthor\nbody\ncreated_utc\nid\nsubreddit\n\n\n\n\n0\nlilfooty\nThis'll hurt them more than the loss\n2019-05-07 21:55:57\nemrz5jp\nsoccer\n\n\n1\nKaeneko\nI loved vampires *so* much, lol. Always fantas...\n2019-05-07 21:34:12\nemrx5eq\nBDSMcommunity\n\n\n2\nLes_Deplorables\nPoor Zombies gonna starve. No Brains!\n2019-05-07 21:21:11\nemrvxjq\nThe_Donald\n\n\n3\nviper2544\nNo one is going to mention the $12 shipping?\n2019-05-07 21:56:45\nemrz8g7\nlegostarwars\n\n\n4\nninjasquirrelarmy\nAgreed. I showed my stylist the Phoenix hair ...\n2019-05-07 21:34:43\nemrx730\nInstagramreality\n\n\n...\n...\n...\n...\n...\n...\n\n\n4794\nm00sedad\nDonald Fucking Trump\n2019-06-19 21:12:28\nerl5gls\nAskReddit\n\n\n4795\nAbramabundiz\nobviously the office or parks, or maybe a spin...\n2019-06-19 21:35:15\nerl7gic\nAskReddit\n\n\n4796\nStarrySkye3\nThat sounds like someone who argues that other...\n2019-06-19 21:33:57\nerl7ccj\notherkin\n\n\n4797\nmostoriginalusername\nI hadn't heard about that one. :)\n2019-06-19 21:41:22\nerl7zzj\ncatsareliquid\n\n\n4798\nggkiyo\nI won't lie, I was someone who said that dota ...\n2019-06-19 21:54:45\nerl96fb\nGames\n\n\n\n\n4799 rows × 5 columns",
    "crumbs": [
      "read data"
    ]
  },
  {
    "objectID": "preprocessing.html",
    "href": "preprocessing.html",
    "title": "preprocessing",
    "section": "",
    "text": "def log_step(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        tic = dt.datetime.now()\n        result = func(*args, **kwargs)\n        time_taken = str(dt.datetime.now() - tic)\n        print(f\"{func.__name__:20} {str(result.shape):10} {time_taken:20}\")\n        return result\n    return wrapper\n\n\nsource\n\n\n\n log_step (func)",
    "crumbs": [
      "preprocessing"
    ]
  },
  {
    "objectID": "preprocessing.html#logging-function",
    "href": "preprocessing.html#logging-function",
    "title": "preprocessing",
    "section": "",
    "text": "def log_step(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        tic = dt.datetime.now()\n        result = func(*args, **kwargs)\n        time_taken = str(dt.datetime.now() - tic)\n        print(f\"{func.__name__:20} {str(result.shape):10} {time_taken:20}\")\n        return result\n    return wrapper\n\n\nsource\n\n\n\n log_step (func)",
    "crumbs": [
      "preprocessing"
    ]
  },
  {
    "objectID": "preprocessing.html#convert-to-lowercase",
    "href": "preprocessing.html#convert-to-lowercase",
    "title": "preprocessing",
    "section": "convert to lowercase",
    "text": "convert to lowercase\n\n@log_step\ndef conv_to_lowerc(comments: pd.DataFrame) -&gt; pd.DataFrame:\n    return comments\\\n            .assign(body=lambda x: x['body'].str.lower())\n\n\nsource\n\nconv_to_lowerc\n\n conv_to_lowerc (comments:pandas.core.frame.DataFrame)\n\n\ntest_lower = pd.DataFrame(\n    columns=['body', 'result'],\n    data=[['Test', 'test'],\n          ['TESTS', 'tests']]\n)\n\n\ntest_lower.pipe(conv_to_lowerc)\n\nconv_to_lowerc       (2, 2)     0:00:00.002159      \n\n\n\n\n\n\n\n\n\n\nbody\nresult\n\n\n\n\n0\ntest\ntest\n\n\n1\ntests\ntests\n\n\n\n\n\n\n\n\n\npdt.assert_series_equal(conv_to_lowerc(test_lower)['body'], test_lower['result'], check_names=False)\n\nconv_to_lowerc       (2, 2)     0:00:00.000774",
    "crumbs": [
      "preprocessing"
    ]
  },
  {
    "objectID": "preprocessing.html#remove-punctuation",
    "href": "preprocessing.html#remove-punctuation",
    "title": "preprocessing",
    "section": "remove punctuation",
    "text": "remove punctuation\n\ntest_rm_punct = pd.DataFrame(\n    columns=['body', 'result'],\n    data=[\n            ['No-punctuation!', 'No punctuation ']\n    ]\n)\n\n\n@log_step\ndef rm_punct(comments: pd.DataFrame) -&gt; pd.DataFrame:\n    return comments\\\n        .assign(body=lambda x: x['body'].str.replace(r'[^\\w\\s]+', ' ', regex=True))\n\n\nsource\n\nrm_punct\n\n rm_punct (comments:pandas.core.frame.DataFrame)\n\n\nrm_punct(test_rm_punct)\n\nrm_punct             (1, 2)     0:00:00.000935      \n\n\n\n\n\n\n\n\n\n\nbody\nresult\n\n\n\n\n0\nNo punctuation\nNo punctuation\n\n\n\n\n\n\n\n\n\npdt.assert_series_equal(\n    rm_punct(test_rm_punct)['body'],\n    test_rm_punct['result'],\n    check_names=False\n)\n\nrm_punct             (1, 2)     0:00:00.000706",
    "crumbs": [
      "preprocessing"
    ]
  },
  {
    "objectID": "preprocessing.html#tokenize",
    "href": "preprocessing.html#tokenize",
    "title": "preprocessing",
    "section": "tokenize",
    "text": "tokenize\n\ntest_tokenize = pd.DataFrame(\n    columns=['body', 'result'],\n    data=[\n            ['These are three-tokens ', ['These', 'are', 'three-tokens']]\n    ]\n)\n\n\n@log_step\ndef tokenize(comments: pd.DataFrame) -&gt; pd.DataFrame:\n    return comments\\\n    .assign(body = lambda x: x['body'].str.split())\n\n\nsource\n\ntokenize\n\n tokenize (comments:pandas.core.frame.DataFrame)\n\n\ntokenize(test_tokenize)\n\ntokenize             (1, 2)     0:00:00.000743      \n\n\n\n\n\n\n\n\n\n\nbody\nresult\n\n\n\n\n0\n[These, are, three-tokens]\n[These, are, three-tokens]\n\n\n\n\n\n\n\n\n\npdt.assert_series_equal(\n    tokenize(test_tokenize)['body'],\n    test_tokenize['result'],\n    check_names=False\n)\n\ntokenize             (1, 2)     0:00:00.000654",
    "crumbs": [
      "preprocessing"
    ]
  },
  {
    "objectID": "preprocessing.html#detect-short-documents",
    "href": "preprocessing.html#detect-short-documents",
    "title": "preprocessing",
    "section": "detect short documents",
    "text": "detect short documents\n\ntest_count_toks = pd.DataFrame(\n    columns=['body', 'result'],\n    data=[\n        [['this' for i in range(5)], 5],\n        [['this' for i in range(20)], 20]\n    ]\n)\n\n\ntest_count_toks\n\n\n\n\n\n\n\n\n\nbody\nresult\n\n\n\n\n0\n[this, this, this, this, this]\n5\n\n\n1\n[this, this, this, this, this, this, this, thi...\n20\n\n\n\n\n\n\n\n\n\ndef count_toks(comments: pd.DataFrame) -&gt; pd.DataFrame:\n    return comments\\\n            .assign(toks=lambda x: x['body'].map(len))\n\n\nsource\n\ncount_toks\n\n count_toks (comments:pandas.core.frame.DataFrame)\n\n\ncount_toks(test_count_toks)\n\n\n\n\n\n\n\n\n\nbody\nresult\ntoks\n\n\n\n\n0\n[this, this, this, this, this]\n5\n5\n\n\n1\n[this, this, this, this, this, this, this, thi...\n20\n20\n\n\n\n\n\n\n\n\n\npdt.assert_series_equal(\n    count_toks(test_count_toks)['toks'],\n    test_count_toks['result'],\n    check_names=False\n)\n\n\ntest_rem_short_comments = pd.DataFrame(\n    columns=['body', 'result'],\n    data=[\n        [['this' for i in range(5)], 'short'],\n    [['this' for i in range(20)], 'long'],\n    ]\n)\n\n\ntest_rem_short_comments\n\n\n\n\n\n\n\n\n\nbody\nresult\n\n\n\n\n0\n[this, this, this, this, this]\nshort\n\n\n1\n[this, this, this, this, this, this, this, thi...\nlong\n\n\n\n\n\n\n\n\n\n@log_step\ndef rem_short_comments(comments: pd.DataFrame, min_toks: int=10) -&gt; pd.DataFrame:\n    return comments\\\n            .pipe(count_toks)\\\n            .query('toks &gt; @min_toks')\\\n            .drop('toks', axis=1)\n\n\nsource\n\n\nrem_short_comments\n\n rem_short_comments (comments:pandas.core.frame.DataFrame,\n                     min_toks:int=10)\n\n\nrem_short_comments(test_rem_short_comments)\n\nrem_short_comments   (1, 2)     0:00:00.004103      \n\n\n\n\n\n\n\n\n\n\nbody\nresult\n\n\n\n\n1\n[this, this, this, this, this, this, this, thi...\nlong\n\n\n\n\n\n\n\n\n\npdt.assert_frame_equal(\n    rem_short_comments(test_rem_short_comments),\n    test_rem_short_comments.query('result == \"long\"'),\n    check_names=False\n)\n\nrem_short_comments   (1, 2)     0:00:00.002157",
    "crumbs": [
      "preprocessing"
    ]
  },
  {
    "objectID": "preprocessing.html#pipeline",
    "href": "preprocessing.html#pipeline",
    "title": "preprocessing",
    "section": "pipeline",
    "text": "pipeline\n\ntest_pipe = pd.DataFrame(\n    columns=['body', 'result', 'flag'],\n    data=[\n            ['This is just a test!', ['this', 'is', 'just', 'a', 'test'], False],\n            ['This is just a much much much much much much much much much much much much much much much much longer test!', ['this', 'is', 'just', 'a', 'much', 'much', 'much', 'much', 'much', 'much', 'much', 'much', 'much', 'much', 'much', 'much', 'much', 'much', 'much', 'much', 'longer', 'test'], True]\n    ]\n)\n\n\ndef clean_comments(comments: pd.DataFrame) -&gt; pd.DataFrame:\n    return comments\\\n            .pipe(conv_to_lowerc)\\\n            .pipe(rm_punct)\\\n            .pipe(tokenize)\\\n            .pipe(rem_short_comments)\n\n\nsource\n\nclean_comments\n\n clean_comments (comments:pandas.core.frame.DataFrame)\n\n\nclean_comments(test_pipe)\n\nconv_to_lowerc       (2, 3)     0:00:00.000878      \nrm_punct             (2, 3)     0:00:00.002117      \ntokenize             (2, 3)     0:00:00.000544      \nrem_short_comments   (1, 3)     0:00:00.002891      \n\n\n\n\n\n\n\n\n\n\nbody\nresult\nflag\n\n\n\n\n1\n[this, is, just, a, much, much, much, much, mu...\n[this, is, just, a, much, much, much, much, mu...\nTrue\n\n\n\n\n\n\n\n\n\ntest_pipe.query('flag == True')['result']\n\n1    [this, is, just, a, much, much, much, much, mu...\nName: result, dtype: object\n\n\n\npdt.assert_series_equal(\n    clean_comments(test_pipe)['body'],\n    test_pipe.query('flag == True')['result'],\n    check_names=False\n)\n\nconv_to_lowerc       (2, 3)     0:00:00.000769      \nrm_punct             (2, 3)     0:00:00.000795      \ntokenize             (2, 3)     0:00:00.000535      \nrem_short_comments   (1, 3)     0:00:00.002410",
    "crumbs": [
      "preprocessing"
    ]
  },
  {
    "objectID": "preprocessing.html#blacklist-for-lexemes",
    "href": "preprocessing.html#blacklist-for-lexemes",
    "title": "preprocessing",
    "section": "blacklist for lexemes",
    "text": "blacklist for lexemes\n\nsource\n\nload_blacklist_lex\n\n load_blacklist_lex (fpath:str='../../blacklist_lex.csv',\n                     propNouns:bool=True)",
    "crumbs": [
      "preprocessing"
    ]
  }
]